# 0. 사용자 인터페이스(챗) 파이프라인 상세

## 1. 개요
- 최상위 엔드포인트는 `POST /api/chat`(Fastify, `server/routes/chat.ts`)이며, 챗봇 오케스트레이터가 GPT-5 Responses API를 통해 답변을 생성한다.
- 기본 모델은 `.env` 의 `CHAT_MODEL`(기본 `gpt-5-mini`)이며, Responses 호출 실패/토큰 초과 시 `CHAT_FALLBACK_MODEL`(`gpt-5-mini` → `gpt-5` 순 등)로 자동 전환한다.
- 모든 LLM 호출은 JSON Schema가 강제된 Responses API를 사용하며, 스트리밍(SSE)과 동기 호출 두 경로를 모두 지원한다.
- 의도 분류, 엔티티 추출, 편집 보조도 모두 Responses API로 통일되어 챗봇과 동일한 재시도/토큰 관리 정책을 따른다.

## 2. 시스템 프롬프트 요약
주 프롬프트는 `server/prompts/chatSystemPrompt.ts`에 정의되어 있으며 다음 책임을 부여한다.

```
You are the project concierge for a translation workflow. Greet warmly, track translation/proofreading/publishing status, and propose actionable next steps (upload origin, start translation, run proofread, export ebook). Mirror the user's tone, keep answers concise, and when suggesting actions provide inline action links like [번역 시작](action:startTranslation).
```

추가로 오케스트레이터는 아래 컨텍스트 메시지를 시스템 메시지로 결합한다.
- 프로젝트 브리프: 제목, 의도, 메모 등 `translationprojects` 메타데이터
- 워크플로우 상태 스냅샷: 번역/교정/품질 단계, Guard 완료 여부 (`statusSummaryBuilder`)
- UI 프론트 스냅샷: 현재 열려 있는 탭, 작업 진행률 (`ProjectContextSnapshotPayload`)
- 라우팅 요약: 의도 분류 결과와 사전 정의된 추천 액션 설명

## 3. 오케스트레이션 흐름 (서버)
참고 파일: `server/routes/chat.ts`

1. **요청 검증 및 선행 기록**
   - 사용자 메시지를 Mongo `chat_messages` 테이블에 저장하여 히스토리 재생성에 사용한다.
   - 프로젝트/사용자 ID로 Postgres/Redis 등 외부 리소스를 한 번만 로드하도록 캐싱.
2. **컨텍스트 수집**
   - 프로젝트 메타, 워크플로우 상태, 번역/교정 진행 상황을 Postgres/Mongo에서 로드해 메시지 컨텍스트(시스템 메시지)로 변환.
   - 프론트에서 전달된 `contextSnapshot`을 이용해 UI 상태(확장된 패널, 진행 중인 작업)를 동기화.
3. **의도 분류**
   - `classifyIntent` (`server/services/intentClassifier.ts`)가 GPT-5-mini Responses API로 intent/confidence를 반환한다.
   - Confidence가 낮거나 최근 실행과 충돌하면 라우팅 레이어가 추천 액션을 조정한다.
4. **라우팅 & 추천**
   - `chatIntentRouter`가 번역 재시작 등 사전 정의된 플로우를 감지하여 synthetic action을 큐잉한다.
   - UI가 표시할 추천 카드(`recommendations`)를 구성한다.
5. **Responses 호출 (메인 답변)**
   - `runJsonSchemaResponse`를 통해 GPT-5 모델을 호출한다. 스트리밍 요청(`stream=true`)인 경우 `responses.stream` → SSE로 전송, 아니면 동기 호출.
   - JSON Schema는 현재 `{ reply: string }` 만을 강제하며, 워크플로우 상태/액션 안내는 별도 UI(LeftSidebar/타임라인)에서 처리한다.
   - 실패 시 토큰 상향 재시도 → fallback 모델 → 사용자에게 친절한 에러 메시지.
6. **후처리 & 데이터 저장**
   - `postProcessChatPayload`는 메시지 텍스트 만을 로깅하고, 의도/추천 결과는 워크플로우 이벤트를 통해 사이드바 UI에 반영한다.
   - 엔티티 추출 Responses 호출은 필요 시(프로젝트 메타 변경)만 진행한다.

## 4. BDM 포인트 (데이터 저장/로그)
- **MongoDB**
  - `chat_messages`: 사용자/어시스턴트 메시지 로그, Actions 기록.
- **PostgreSQL**
  - `translationprojects`: 프로필 업데이트(제목/저자/컨텍스트) 저장.
  - `chat_intent_snapshot`: 최근 의도 분류 결과 캐싱.
- **Observability 로그**
  - Fastify logger가 모델명, 재시도 횟수, 토큰 사용량, fallback 여부를 출력.
  - SSE 메타 정보(`chat.complete.meta`)를 프론트가 수집해 디버깅 가능.

## 5. LLM 호출 매커니즘
- **모델 선택**: `getChatResponsesDefaults()`가 `.env` 값을 읽어 모델, verbosity, reasoning effort, 토큰 상한을 결정하고 fallback 모델을 제공한다.
- **JSON Schema**: Responses API `text.format = json_schema`로 강제. 파싱 실패 시 SyntaxError로 재시도.
- **재시도/토큰 증액**: `runResponsesWithRetry`가 `openai_response_incomplete` 또는 JSON 파싱 실패 시 토큰을 1.5배씩 증액해 재호출한다.
- **엔티티 추출**: 메인 응답과 별개로 `chat_entity_payload_v1` 스키마로 GPT-5-mini를 호출해 제목/저자/메모 업데이트.
- **의도 분류/편집 보조**: 각각 전용 스키마(`chat_intent_payload_v1`, `chat_editing_payload_v1`)를 사용하며 동일한 재시도 정책을 공유한다.

## 6. 스트리밍 & 프런트 연동
- 서버는 SSE 이벤트(`Transfer-Encoding: chunked`)로 아래 이벤트를 전송한다.
  - `chat.delta`: 부분 텍스트
  - `chat.complete`: 최종 메시지 + 토큰 메타
  - `chat.error`: 오류 메시지
  - `chat.end`: 스트림 종료 신호
- 프런트(`web/src/services/api.ts:chatStream`)는 `ReadableStream`을 파싱해 메시지를 점진적으로 렌더링하고, 완료 시 좌측 사이드바의 Quick Action 및 워크플로우 타임라인과 싱크만 수행한다. GPT 응답에서 내려오는 액션 JSON은 더 이상 사용하지 않는다.
- 메시지 상태는 `web/src/hooks/useChatMessages.ts`의 전용 훅이 단일 소스로 보관한다. 히스토리 로딩은 `syncHistory`가 ID/본문을 기준으로 병합하고, 사용자·어시스턴트 메시지는 낙관적 ID(`clientId`)로 추가한 뒤 스트림 완료 시 서버 ID로 치환한다.
- 스트림 실패 시 자동 폴백 대신 사용자가 재전송을 시도하도록 UX를 정비한다.

## 7. 환경 변수 & 설정 요약
- `.env` (server)
  - `CHAT_MODEL`, `CHAT_FALLBACK_MODEL`
  - `CHAT_VERBOSITY`, `CHAT_REASONING_EFFORT`, `CHAT_MAX_OUTPUT_TOKENS`, `CHAT_ENTITY_MODEL`, `CHAT_ENTITY_MAX_OUTPUT_TOKENS`
  - `INTENT_CLASSIFIER_MODEL`, `INTENT_CLASSIFIER_VERBOSITY`, `INTENT_CLASSIFIER_EFFORT`
  - `EDITING_ASSIST_MODEL`, `EDITING_ASSIST_MAX_OUTPUT_TOKENS`
- 스키마 정의: `server/services/responsesSchemas.ts`
- 공통 헬퍼: `server/services/responsesRunner.ts`, `server/services/openaiClient.ts`

## 8. 향후 과제 (백로그)
- **대화 로그 분석**: `chat.complete.meta`의 토큰/latency를 집계하여 Admin 대시보드에 시각화.
- **다중 턴 메모리**: 프로필 업데이트 외에도 장기 대화 컨텍스트를 Redis 등 외부 저장소와 연동.
- **스트리밍 오류 복구**: SSE 오류 시 클라이언트가 자동 재구독하도록 백오프 전략 추가.
- **자동화 테스트**: 챗 라우터/의도 분류/엔티티 추출을 커버하는 통합 테스트 도입.
