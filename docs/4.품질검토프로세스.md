# 4. 품질 검토(Quality Assessment) 파이프라인

## 1. 개요
- 품질 검토는 `/api/evaluate`(sync) 또는 `/api/evaluate/stream`(SSE) 엔드포인트에서 시작되며, 내부적으로 `evaluateQuality` / `evaluateQualityStream` ( `server/agents/qualityAgent.ts` )을 호출한다. 이 모듈이 TDM(Task/Decision Manager) 역할을 수행한다.
- 입력으로 원문(KO) / 번역문(EN) / 선택적인 작가 의도를 받아, GPT‑5 Responses API를 이용해 정량(점수)·정성(코멘트) 피드백을 생성한다.
- 결과와 청크 메타데이터는 Mongo `quality_assessments` 컬렉션에 저장되며, 프로젝트/잡 메타(`projectId`, `jobId`)가 주어지면 원문처리/번역 단계와 연결되는 BDM(Baseline/Data Manager) 역할을 한다.

## 2. 시스템 프롬프트
Quality 에이전트는 고정된 시스템 프롬프트(`SYSTEM_PROMPT`, `server/agents/qualityAgent.ts:406-470`)를 사용한다. 핵심 내용은 다음과 같다.

```
You are a professional literary translator and editor.
Evaluate a single English translation of a Korean literary source, producing BOTH numeric scores (0–100) and concise literary commentary in BOTH Korean and English.

Follow this fixed output schema (JSON). Do NOT include any text outside JSON.
Schema:
{
  "overallScore": number,
  "qualitative": {
    "emotionalDepth": { "ko": string, "en": string },
    "vividness": { "ko": string, "en": string },
    "metaphors": { "ko": string, "en": string },
    "literaryValue": { "ko": string, "en": string }
  },
  "quantitative": {
    "Fidelity": { "score": number, "commentary": { "ko": string, "en": string } },
    "Fluency": { ... },
    "Literary Style": { ... },
    "Cultural Resonance": { ... },
    "Creative Autonomy": { ... }
  }
}
```

- 각 코멘트는 35단어 이하, JSON 이외의 텍스트는 금지.
- 응답 누락 시 재시도하며, 반복 실패 시 일부 항목에 fallback 코멘트를 삽입한다.

## 3. TDM(Task/Decision Manager) 흐름
1. **입력 파싱/옵션 결정** (`server/routes/evaluation.ts:600-709`)
   - 사용자 입력을 검증하고 `EvaluateParams`를 구성한다. 모델 미지정 시 `.env`의 `LITERARY_QA_MODEL`(기본 `gpt-5-mini`)이 사용된다.
   - `maxCharsPerChunk`, `overlap`, `concurrency` 등의 옵션을 적용한다.
2. **워크플로우/Job 관리** (`server/routes/evaluation.ts:717-821`)
   - 워크플로우 런이 존재하면 상태를 갱신하고, 실패 시 `jobs` / `workflow_runs`를 실패 처리한다.
3. **문장 정렬** (`server/agents/quality/alignedPairs.ts`)
   - 프로젝트/잡 메타가 있으면 최종 번역 세그먼트(또는 최신 Draft)에서 정렬 데이터를 재사용한다.
   - 그렇지 않으면 KO/EN 텍스트를 문장 분할 후 인덱스 기반으로 매칭한다.
4. **청크 생성** (`server/agents/quality/chunking.ts`)
   - `tokenBudget ≈ maxCharsPerChunk / 3.5` 을 이용해 연속 문장 페어를 묶고, 오버랩 토큰 수만큼 이전 문장을 공유한다.
   - 청크 메타에는 문장 범위, 토큰 수, 정렬 소스(segments/draft/manual)가 기록된다.
5. **LLM 호출** (`server/agents/qualityAgent.ts:824-1100`)
   - 각 청크는 `runResponsesWithRetry`를 통해 모델을 호출한다.
   - `text.format`에 JSON Schema(`evalResponseJsonSchema`)를 strict 모드로 지정해 스키마 일탈을 방지한다.
   - `max_output_tokens`는 기본 `LITERARY_QA_MAX_OUTPUT_TOKENS` (기본 12k)에서 시작해, truncation 시 1.5~2배까지 자동 증액된다.
   - GPT‑5 모델일 경우 `LITERARY_QA_MODEL_VERBOSITY`, `LITERARY_QA_MODEL_REASONING_EFFORT` 환경변수가 있으면 적용된다.
6. **결과 집계** (`server/agents/qualityAgent.ts:1408-1450`)
   - 청크별 결과를 토큰 가중 평균으로 합산해 overallScore, 정량/정성 코멘트를 생성한다.
   - 부족한 필드를 보완하고, 청크 메타(토큰, latency, truncation, attempts, requestId 등)를 `meta.chunkStats`로 기록한다.
7. **저장/응답**
   - Sync 모드: `FinalEvaluation`을 반환하고 `quality_assessments` 컬렉션에 저장한다.
   - Streaming: 진행 이벤트(`chunk-start`, `chunk-partial`, `chunk-complete`, `progress`, `complete`, `error`)를 SSE로 전송한다 (`web/src/hooks/useQualityAgent.ts`).

## 4. BDM(Baseline/Data Manager) 포인트
- **MongoDB**
  - `quality_assessments`: overallScore, 정량/정성 코멘트, 청크 메타, 모델, 토큰 정보를 저장. 프로젝트/잡 식별자를 통해 번역/교정 단계와 연계된다.
- **PostgreSQL**
  - `jobs`, `workflow_runs`: 품질 평가 워크플로우 상태를 관리하며, 실패 시 원인을 기록한다.
  - 별도 로그 테이블은 아직 없으나, Admin 페이지 연동 시 참고할 수 있도록 chunkStats에 requestId/모델/토큰이 담겨 있다.
- **프런트엔드**
  - 스트리밍 모드에서 NDJSON 이벤트로 진행 상황을 노출 (`web/src/hooks/useQualityAgent.ts:359-476`).
  - 품질 결과는 UI에서 그래프/코멘트로 표시되며, 필요 시 `overallScore` 기반으로 pass/fail 정책을 적용할 수 있다.

## 5. 모델/토큰 설정
| 항목 | 기본값 | ENV | 비고 |
| --- | --- | --- | --- |
| 기본 모델 | `gpt-5-mini` | `LITERARY_QA_MODEL` | 요청에 `model`을 지정하면 우선. |
| Verbosity | 없음 | `LITERARY_QA_MODEL_VERBOSITY` | GPT‑5 계열만 적용. |
| Reasoning Effort | 없음 | `LITERARY_QA_MODEL_REASONING_EFFORT` | GPT‑5 계열만 적용. |
| `max_output_tokens` | 12,000 | `LITERARY_QA_MAX_OUTPUT_TOKENS` | truncation 시 자동 증액 (cap은 `LITERARY_QA_MAX_OUTPUT_TOKENS_CAP`). |
| 청크 크기 | 3,200자 | `LITERARY_QA_CHUNK_SIZE` | 토큰 예산 산정에 사용. |
| 청크 오버랩 | 200자 | `LITERARY_QA_CHUNK_OVERLAP` | 문장 중간 분할 방지. |
| 동시성 | 2 | `LITERARY_QA_CONCURRENCY` | `evaluateQuality` 옵션으로도 오버라이드 가능. |

## 6. 모니터링
- 로그: 모델 선택/정렬 방식/청크 정보/재시도/최종 스코어 등 주요 단계에서 `console.log`로 기록된다.
- 오류 처리: 청크 실패(`chunk-error`) 또는 최종 실패(`error`) 이벤트로 클라이언트에 전달되며, 서버는 워크플로우 런을 실패 상태로 업데이트한다.
- 추후 Admin KPI: truncation율, 평균 latency, fallback 비율 등 chunkStats 기반 지표를 대시보드에 추가하면 튜닝 효과를 쉽게 확인할 수 있다.

## 7. 향후 개선 아이디어
1. **타겟 모델 계층화**: 현재는 단일 모델만 사용하므로, Proofreading처럼 `mini → pro` 순으로 fallback하여 비용/지연을 줄일 여지가 있다.
2. **청크 어댑티브 재시도**: truncation이 반복되는 청크는 자동으로 분할하거나 텍스트 길이를 줄이는 로직 도입.
3. **Evidence 강화**: Fidelity 점수가 낮을 때 자동으로 원문/번역 근거 문장을 반환하도록 스키마 확장.
4. **관측성 확장**: Admin 페이지에 품질 평가 로그(토큰, 모델, 소요 시간)를 추가해 추세를 추적한다.

---
**Owner**: Quality Evaluation Team  
**Last updated**: 2025-10

