# 품질 검토 프로세스 상세

## 1. 개요
- 사용자는 `/api/evaluate` 또는 `/api/evaluate/stream` 엔드포인트로 원문과 번역문을 전송해 품질 평가를 요청한다 (`server/routes/evaluation.ts:596-714, 717-826`).
- 서버는 `evaluateQuality` / `evaluateQualityStream` 호출을 통해 GPT 기반 품질 평가 에이전트를 실행한다 (`server/routes/evaluation.ts:686-707, 781-813`).
- 평가 결과는 LLM이 생성한 정량/정성 피드백과 토큰 사용량, 청크 메타데이터를 포함하며, 최종적으로 `QualityAssessment` 컬렉션에 저장된다 (`server/routes/evaluation.ts:805-821`).

## 2. 입력 및 구성 옵션
품질 평가 요청 본문(`server/routes/evaluation.ts:600-641`)에는 다음 필드가 포함된다.
| 필드 | 설명 |
| --- | --- |
| `source` | 한국어 원문 전체 텍스트 |
| `translated` | 대응하는 영어 번역문 전체 텍스트 |
| `authorIntention` | 선택 입력. 번역 방향성/작가 의도를 전달 |
| `model` | 선택 입력. 기본은 `gpt-5-mini` |
| `maxCharsPerChunk` | 청크 최대 길이(기본 3200자). 토큰 예산 산정에 사용 |
| `overlap` | 청크 사이 오버랩 문자 수(기본 200자) |
| `projectId`, `jobId` | 번역 초안/세그먼트와 연결하기 위한 메타데이터 |
| `workflowLabel`, `workflowAllowParallel`, `concurrency` | 워크플로우/병렬 실행 제어 옵션 |

## 3. 정렬 및 청크 생성
품질 평가 에이전트는 입력 텍스트를 평가 가능한 청크로 묶기 전에 원문-번역 문장을 맞춘다 (`server/agents/quality/alignedPairs.ts`, `server/agents/quality/chunking.ts`).
1. **세그먼트 데이터 재사용**: `projectId`와 `jobId`가 모두 제공되고, 해당 Job에 대한 최종 번역 세그먼트가 존재하면 `TranslationSegment`(variant='final') 정보를 가져와 KO/EN 문장을 정밀하게 정렬한다.
2. **번역 Draft fallback**: 최종 세그먼트가 없을 때는 최근 번역 Draft(`TranslationDraft.segments`)에서 origin/translation 쌍을 재구성한다.
3. **문장 기반 fallback**: 위 두 조건이 모두 실패하면 문장 분할(`splitSentencesByLang`) 후 같은 인덱스끼리 매칭하는 간단한 방식으로 폴백한다.

정렬된 문장 페어는 토큰 예산 기반 청크로 묶인다 (`buildQualityChunks`).
- `tokenBudget = maxCharsPerChunk / 3.5`를 근사 토큰 상한으로 사용하며, 연속 문장 페어를 더해가다가 예산을 넘으면 새 청크를 시작한다.
- 청크 간 오버랩은 이전 청크의 마지막 문장 페어에서 토큰 기준으로 역추적해 일부 문장을 공유한다 (문장 중간 분할 방지).
- 각 청크는 KO/EN 문장 개수, 토큰 수, 시작/끝 문장 인덱스 메타데이터를 기록한다. 이 정보는 로그와 스트림 이벤트로 노출된다.

## 4. 품질 평가 LLM 호출 구조
### 4.1 시스템 및 사용자 프롬프트
- 시스템 프롬프트는 문학 전문 평가자 역할을 부여하고, 아래 JSON 구조로만 응답하도록 강하게 요구한다 (`server/agents/qualityAgent.ts:430-467`). 주요 요구 사항:
  - `overallScore`(0~100), `qualitative`(감정/생동감/비유/문학적 가치 각 한/영 코멘트), `quantitative`(충실성/유창성/문체/문화적 공명/창의적 자율성 점수 및 양언어 코멘트) 필드 강제.
  - 영어/한국어 코멘트 각각 단문 35단어 이하.
  - JSON 외의 서술 금지.
- 사용자 프롬프트는 청크별로 `SOURCE_TEXT (Korean)` / `TRANSLATED_TEXT (English)` / `AUTHOR_INTENTION`(선택) / 재응답 시 강조 문구를 포함한다 (`server/agents/qualityAgent.ts:850-915`).

### 4.2 Responses API 호출
- 모델은 기본 `gpt-5-mini`. 환경변수로 오버라이드 가능 (`server/agents/qualityAgent.ts:202-229`).
- `max_output_tokens`는 청크당 기본 6000, truncation 발생 시 단계적으로 증가 (`server/agents/qualityAgent.ts:1049-1089`).
- `text.format`에서 JSON Schema(`evalResponseJsonSchema`)를 지정하고 `strict: true` 옵션으로 schema 준수를 강제한다 (`server/agents/qualityAgent.ts:876-917`).
- GPT-5 모델일 경우 verbosity/effort도 환경변수에 따라 설정된다 (`server/agents/qualityAgent.ts:220-223, 893-900`).
- 응답 수집: `safeExtractOpenAIResponse`가 Responses API 결과에서 parsed JSON 파트 또는 텍스트 파트를 추출하고, 타입 오류 시 재시도 로직이 동작한다 (`server/services/llm.ts:37-95`, `server/agents/qualityAgent.ts:932-1093`).

### 4.3 재시도 전략
- `openai_response_incomplete` (토큰 부족) → 토큰 예산 2배로 증가 후 재시도.
- JSON 파싱 실패 → 토큰 예산 1.5배로 증가 후 재시도.
- 최대 재시도 횟수 이후에도 성공하지 못하면 오류를 던져 스트림에 `chunk-error`로 전달한다.

## 5. 결과 집계 및 메타데이터
### 5.1 정량/정성 통합
- 청크별 응답은 `ChunkEvaluationRecord` 구조에 저장되며, 토큰 수·문장 페어 범위·재시도 횟수·fallback 여부 등이 보존된다 (`server/agents/qualityAgent.ts:1074-1180`).
- 전체 평가 점수는 토큰 수를 가중치로 한 평균으로 계산된다 (`server/agents/qualityAgent.ts:1408-1450`). 정성 코멘트는 조합해서 최종 응답을 구성한다 (`server/agents/qualityAgent.ts:1424-1439`).

### 5.2 청크 통계 및 스트리밍 이벤트
- 스트리밍 모드에서는 평가 진행 상황이 NDJSON 이벤트로 전송된다 (`web/src/services/api.ts:48-120`, `web/src/hooks/useQualityAgent.ts:359-476`).
  - `chunk-start`: 각 청크의 길이, 토큰 수, 문장 페어 수 등.
  - `chunk-partial`: 재시도 시 누락 필드 리스트.
  - `chunk-complete`: 처리 시간, 공유된 메타데이터, 모델 요청 ID.
  - `progress`: 완료된 청크 수.
  - `complete`: 최종 결과.
- 접속이 끊기거나 오류 발생 시 `chunk-error` 또는 `error` 이벤트가 전달되며, 서버는 로그를 남기고 워크플로우 런을 실패 처리한다 (`server/routes/evaluation.ts:727-760`).

### 5.3 최종 저장
- 스트리밍/비스트리밍 모드 모두 `evaluateQuality` 반환 결과를 `QualityAssessment` 컬렉션에 저장한다 (`server/routes/evaluation.ts:805-821`).
  - 저장 정보: overall score, 각 항목 점수/코멘트, 청크 메타(토큰 사용, 모델, 요청 ID 등), 원문/번역 텍스트.
  - `projectId`/`jobId`가 지정된 경우 `translationprojects`와 연계된 품질 내역으로 활용된다.

## 6. 로그 및 모니터링 포인트
- 주요 로그 예시 (`server/agents/qualityAgent.ts`):
  - 시작/모델 선택/정렬 방식(`draft` vs `segment` vs fallback) 출력.
  - 청크 생성 시 토큰 수와 문장 페어 수.
  - LLM 호출 전 “🤖 Calling OpenAI…”, 응답 수신 직후 “📤 response received”.
  - JSON 파싱 성공/실패 여부, 재시도 상황.
  - 최종 결과 요약(`overallScore`, 처리 청크 수, request ID 개수).
- 스트리밍 모드에서는 프런트엔드에서 청크별 진행 상황과 오류 메시지를 실시간으로 수신할 수 있다 (`web/src/hooks/useQualityAgent.ts:359-476`).
- 품질 평가 실패 시 `jobs` 테이블 상태와 워크플로우 런이 실패로 마킹되며, 사용자는 UI에서 실패 메시지를 확인하게 된다 (`server/routes/evaluation.ts:742-793`).

## 7. 향후 확장 포인트
- 문장 정렬 개선: 현재 fallback은 문장 수를 단순히 맞춰보는 방식이므로, embedded alignment 등 보다 견고한 알고리즘을 적용하면 “원문 미대응” 오탐을 줄일 수 있다.
- Micro-check 연계: 장차 길이 외 품질 지표(예: 용어 일치, 숫자/단위 체크)를 Micro-check 단계와 공유해 품질 평가 결과를 보완할 수 있다.
- 품질 증거 필드 강화: Fidelity가 누락을 주장할 경우 근거 문장을 추가로 요구하도록 schema 확장을 고려할 수 있다.

---
**담당**
- Owner: Quality Evaluation Team
- 최신 업데이트: 2025-??-??
