# 품질 검토 프로세스 (Quality Assessment)

## 1. Purpose & Scope
- 번역본의 문학적 품질을 정량/정성 지표로 평가하고, 후속 의사결정(승인/재번역)에 활용할 수 있는 리포트를 생성한다.
- TDM은 `evaluateQuality`/`evaluateQualityStream` 로직이 담당하며, 청크 분할→LLM 평가→집계를 조율한다.
- BDM은 Mongo/Postgres 저장소에 평가 결과와 메타를 보존해 프로젝트 타임라인·Admin 보고서·Chat 어시스턴트가 동일 데이터를 참조하게 한다.

## 2. Trigger & Entry Points
- Sync: `POST /api/evaluate` 는 전체 평가가 끝난 후 JSON 결과를 반환한다 (`server/routes/evaluation.ts:600-709`).
- Streaming: `POST /api/evaluate/stream` 은 NDJSON 이벤트(`start`, `chunk-*`, `progress`, `complete`, `error`)를 전송한다 (`server/routes/evaluation.ts:710-1100`).
- 클라이언트는 `api.runQualityStream` 을 사용하며, Quality 탭/Chat 플로우에서 동일 로직을 공유한다 (`web/src/services/api.ts:2093-2160`).

## 3. Environment & LLM Configuration (.env)
| Variable | Default | Purpose | Notes |
| --- | --- | --- | --- |
| `LITERARY_QA_MODEL` | `gpt-5-mini` | 기본 평가 모델 | 운영에서는 `gpt-5` 로 상향해 심층 분석 제공.
| `LITERARY_QA_MODEL_VERBOSITY` | unset | Responses `text.verbosity` | `low/medium/high`. unset 시 모델 기본값 활용.
| `LITERARY_QA_MODEL_REASONING_EFFORT` | unset | Responses `reasoning.effort` | `minimal/low/medium/high`.
| `LITERARY_QA_MAX_OUTPUT_TOKENS` | `12000` | 초기 `max_output_tokens` | truncation 시 CAP까지 1.5~2배 증가.
| `LITERARY_QA_MAX_OUTPUT_TOKENS_CAP` | `24000` | 토큰 CAP | 초과 시 청크를 분할하거나 실패 처리.
| `LITERARY_QA_CHUNK_SIZE` | `3200` | 청크 분할 기준(문자) | 오버랩과 함께 토큰 예산 계산.
| `LITERARY_QA_CHUNK_OVERLAP` | `200` | 청크 오버랩 | 문장 분단 시 맥락 유지.
| `LITERARY_QA_CONCURRENCY` | `2` | 동시에 평가할 청크 수 | 스트리밍 모드에서 Progress 체감과 비용 균형.

## 4. Workflow Orchestration (TDM)
- 입력 검증 후 `EvaluateParams` 를 구성하고, 프로젝트/Job 메타를 로드한다 (`server/routes/evaluation.ts:600-821`).
- 문장 정렬(`alignedPairs.ts`)과 청크 생성(`chunking.ts`)으로 LLM 호출 단위를 만든다.
- 각 청크를 Responses API로 평가하고, 결과를 `chunkStats`에 즉시 기록한다 (`server/agents/qualityAgent.ts:824-1100`).
- 모든 청크 완료 후 토큰 가중 평균으로 overallScore 및 정성 코멘트를 생성한다 (`server/agents/qualityAgent.ts:1408-1450`).

## 5. LLM Invocation Strategy & Safeguards
- Responses API + JSON Schema(`evalResponseJsonSchema`)를 사용해 점수/코멘트 구조를 강제한다.
- truncation 감지 시 `max_output_tokens`를 최대 CAP까지 증가시키고, 필요한 경우 청크 길이를 자동 조정한다.
- 모델 오류/429 시 동일 모델로 재시도하며, 실패 청크는 `chunk-error` 이벤트로 스트리밍되어 UI에 즉시 노출된다.
- Reasoning Effort/Verbosity는 `.env` 설정을 따르며, 고비용 설정은 운영 환경에서만 제한적으로 사용한다.

## 6. Data Persistence & BDM Responsibilities
- Mongo `quality_assessments` 컬렉션에 overallScore, 정량/정성 결과, 청크 통계, 모델/토큰 메타를 저장한다.
- Postgres `jobs`/`workflow_runs` 는 품질 워크플로우 상태와 오류를 기록한다.
- 평가 결과는 프로젝트 메타와 연결되어 Chat 어시스턴트, 타임라인, Admin 보고서에서 재사용된다 (`server/routes/evaluation.ts:1408-1507`).

## 7. UX Integration & Status Delivery
- `useQualityAgent` 는 NDJSON 이벤트(`start`, `chunk-start`, `chunk-retry`, `chunk-partial`, `chunk-complete`, `chunk-error`, `progress`, `complete`, `error`)를 처리해 `useWorkflowStore.quality` 상태를 갱신한다 (`web/src/hooks/useQualityAgent.ts`).
- Right Panel 품질 카드와 타임라인에 진행률, 현재 청크, 최종 점수가 표시되며, 오류 시 즉시 경고한다 (`web/src/components/layout/RightPanel.tsx`).
- Chat Assistant는 평가 진행/결과를 자연어로 요약하고 후속 액션(재번역, 교정 요청 등)을 제안한다 (`server/routes/chat.ts:1398-1515`).

## 8. Reliability & Performance Controls
- 청크 오버랩과 토큰 예산 계산으로 긴 문서를 여러 청크로 나눠 안정적으로 처리한다.
- `LITERARY_QA_CONCURRENCY` 로 동시 호출 수를 제한해 OpenAI rate limit과 응답 시간을 균형 있게 유지한다.
- 실패 청크는 즉시 `chunk-error` 로 보고되고, 필요 시 수동 재평가(특정 청크만 재시도)가 가능하다.

## 9. Monitoring & Alerting
- `[QUALITY]` 로그가 단계별로 남으며 chunkStats에는 requestId, 모델, 토큰, 시도 횟수가 포함된다.
- Mongo `quality_assessments.meta.chunkStats` 데이터를 기반으로 truncation 비율, 평균 latency, fallback 횟수를 대시보드에서 추적할 수 있다.
- `recordTokenUsage` 는 `event_type='quality'` 로 토큰/비용을 집계한다.

## 10. Risks & Next Actions
- 24k 토큰 CAP에도 truncation이 발생하는 초장편 텍스트는 사전 요약 또는 청크 축소 전략을 도입해야 한다.
- 모델 단일화로 인해 fallback 다양성이 부족하므로, `LITERARY_QA_MODEL_FALLBACK` 지원을 추가 고려한다.
- UX에서 chunk-error를 재시도할 수 있는 버튼을 제공해 운영 개입 없이 복구 가능한 흐름을 준비한다.
