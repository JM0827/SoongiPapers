# 원문 처리 프로세스 상세

## 1. 업로드 진입점
- 엔드포인트: `PUT /api/projects/:projectId/origin`는 인증·요금제 확인 이후에만 실행된다 (`server/index.ts:969`).
- 요청 방식
  - **멀티파트 파일 업로드**: `req.isMultipart()` 분기에서 단일 파일을 받고 `jobId` 등 부가 필드를 읽어들인다 (`server/index.ts:1072`).
  - **직접 텍스트 업로드**: JSON 본문의 `content` 문자열을 그대로 저장할 수 있는 경로를 제공한다 (`server/index.ts:1157`).
- 요청 거부 조건
  - 파일 미첨부, 문자열 이외의 `content`, 추출 실패, 지원하지 않는 확장자, 10MB 초과 파일 등은 4xx/5xx 에러로 응답된다 (`server/services/origin/extractor.ts:82`, `server/index.ts:1093`).

## 2. 파일 확장자별 파싱 파이프라인
`extractOriginFromUpload`가 확장자를 기준으로 추출기를 선택하고 텍스트를 정제한다 (`server/services/origin/extractor.ts:72`).

| 확장자 | 추출기 | 외부 라이브러리 & 방식 | 비고 |
| --- | --- | --- | --- |
| `.txt` | `plain` | Buffer를 UTF-8로 디코드 | 기본값 (`server/services/origin/extractor.ts:95`)
| `.docx` | `mammoth` | `mammoth.extractRawText`로 XML 기반 DOCX 추출 | 스타일 무시 (`server/services/origin/extractor.ts:100`)
| `.doc` | `word-extractor` | 임시 파일 생성 후 WordExtractor로 본문 추출 | 완료 후 파일 제거 (`server/services/origin/extractor.ts:170`)
| `.pdf` | `pdf-parse` | Lazy-load하여 텍스트 레이어 추출 | 모듈 함수 캐시 (`server/services/origin/extractor.ts:181`)
| `.epub` | `epub2` | 각 챕터 HTML을 파싱해 태그 제거 후 줄바꿈 정리 | 엔티티 디코드 포함 (`server/services/origin/extractor.ts:195`)
| `.hwp/.hwpx` | `hwp-extractor` | Python 스크립트 `hwp_extractor.py` 실행, pyhwp 또는 hwp-extract 사용 | Python 타임아웃 30초 (`server/services/origin/extractor.ts:214`, `server/services/origin/hwp_extractor.py:1`)

### HWP 처리 스크립트
- `hwp_extractor.py`는 PyHWP와 `hwp_extract` 모듈을 우선 사용하며, 다국어 인코딩 판별·정규화, 유니코드 필터링, 행 간격 정리를 수행한다 (`server/services/origin/hwp_extractor.py:41`).
- 패키지가 없거나 타임아웃이 발생하면 사용자 친화적인 오류 메시지를 반환한다 (`server/services/origin/hwp_extractor.py:78`, `server/services/origin/extractor.ts:236`).

## 3. 텍스트 정리 및 통계 산출
- 추출 결과는 `cleanText` 유틸을 통해 PDF 하드랩 제거, 불필요 공백 정규화, 0-width 문자 제거 등을 수행한다 (`server/services/origin/extractor.ts:127`, `server/utils/cleanText.ts:11`).
- `normalizeExtractedText`는 개행과 제어 문자, ZWJ 범위를 재정리하고 3연속 이상의 줄바꿈을 2개로 축약한다 (`server/services/origin/extractor.ts:155`).
- 통계: 문자수·단어수를 계산하여 메타데이터에 저장하며, 이후 Mongo 문서와 응답에 포함된다 (`server/services/origin/extractor.ts:137`).

## 4. 원문 저장 경로
- **MongoDB `origin_files`**: 파일별 메타데이터(파일명, 확장자, 추출기, word/char count, 바이너리)를 upsert한다 (`server/index.ts:998`).
- **MongoDB `translation_files`**: 최신 번역 파일 문서에 원문 텍스트/파일 정보를 동기화해 후속 파이프라인이 동일 소스에 접근하도록 한다 (`server/index.ts:1021`).
- **Postgres `translationprojects.origin_file`**: Mongo ObjectId를 문자로 저장하여 프로젝트 스냅샷에서 원문 ID를 추적한다 (`server/index.ts:1041`).
- 모든 경로는 저장 시점을 `updated_at`으로 갱신하고, 응답에 정규화된 콘텐츠와 메타데이터를 돌려준다 (`server/index.ts:1118`).

## 5. 자동 분석 & 문서 프로파일
1. **Job 등록**: 업로드 완료 후 로그인 사용자인 경우 `enqueueProfileAnalysisJob`이 `profile` 타입 BullMQ(?) 대신 Postgres 기반 큐에 작업을 넣는다 (`server/index.ts:1052`).
2. **워커 루프**: `startProfileWorker`가 2.5초 간격으로 큐에서 작업을 가져와 실행 상태를 `running`으로 조정한다 (`server/index.ts:2666`).
3. **원문/번역 선택**: `variant`가 `origin`이면 `origin_files`에서 텍스트를 로드하고, 없으면 최신 문서를 fallback으로 사용한다 (`server/index.ts:2724`).
4. **LLM 호출**: `analyzeDocumentProfile`가 OpenAI Chat Completions API를 호출한다 (`server/agents/profile/profileAgent.ts:193`).
   - 모델: 기본 `gpt-4o-mini`, 환경 변수로 교체 가능 (`server/agents/profile/profileAgent.ts:6`).
   - 시스템 프롬프트: 문학 비평가 관점의 요약과 번역 노트 수집을 지시 (`server/agents/profile/profileAgent.ts:220`).
   - 사용자 프롬프트: 요약, 의도, 독자 포인트, Translation Notes JSON 구조를 명시 (`server/agents/profile/profileAgent.ts:233`).
   - 파라미터: `temperature` 0.5, `response_format` 강제 JSON (`server/agents/profile/profileAgent.ts:251`).
   - 컨텍스트 제한: 8,000자 초과 시 앞부분만 사용하고 차단 표시 (`server/agents/profile/profileAgent.ts:213`).
5. **후처리**: 결과 JSON을 파싱해 스토리 요약, 의도, 독자 포인트를 120/60/35 단어로 잘라내고, 번역 참고 항목을 구조화한다 (`server/agents/profile/profileAgent.ts:263`).
6. **DocumentProfile 저장**: 버전 번호를 증가시키며 `document_profiles`에 summary/metrics/translation_notes/소스 해시를 기록한다 (`server/index.ts:2802`).
7. **토큰 사용 로깅**: `recordTokenUsage`가 프로젝트 단위로 입력/출력 토큰을 합산해 비용 분석을 지원한다 (`server/index.ts:2838`).

## 6. 번역 준비용 세그멘테이션
- 번역 파이프라인을 시작할 때 `segmentOriginText`가 원문을 문단/문장 구분으로 분할해 `OriginSegment` 목록을 만든다 (`server/agents/translation/segmentationAgent.ts:106`).
- 기본 모드와 세그먼트 길이는 설정 파일에서 가져오며, 너무 긴 문장·문단은 2,000자 단위로 문장 경계 근처에서 분할한다 (`server/agents/translation/segmentationAgent.ts:28`).
- 결과에는 SHA-256 원문 해시와 각 세그먼트의 문단·문장 인덱스가 포함되어 번역 스테이지가 정렬 정보를 유지한다 (`server/agents/translation/segmentationAgent.ts:171`).

## 7. 데이터 흐름 요약
1. 사용자 업로드 → 확장자별 추출 및 정제.
2. Mongo `origin_files` 및 연계된 `translation_files`, Postgres `translationprojects`에 동시 업데이트.
3. 프로파일 워커가 큐에서 작업을 가져가 LLM 분석을 수행하고, `document_profiles`에 스냅샷을 저장.
4. 번역 요청 시 세그멘테이션 결과와 번역 노트를 공유해 Literal/Style/Emotion 단계가 동일한 원문 컨텍스트로 작업.

## 8. 주요 제약 & 에러 메시지
- 최대 파일 크기 10MB (`server/services/origin/extractor.ts:21`).
- 지원 확장자 외 업로드 시 400 에러 (`server/services/origin/extractor.ts:86`).
- 텍스트 추출 실패 시 “No textual content…” 응답 (`server/index.ts:1136`).
- Python 의존성 부재나 타임아웃 시 500 에러와 안내 메시지 (`server/services/origin/extractor.ts:234`).

以上 프로세스를 통해 업로드된 원문은 일관된 정제 과정을 거쳐 저장되고, 자동 LLM 분석을 통해 번역 노트까지 생성되어 이후 번역·교정 파이프라인과 연결된다.
