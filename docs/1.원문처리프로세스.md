# 원문 처리 프로세스 상세 (Origin Pipeline)

## 1. 업로드 진입점
- 엔드포인트: `PUT /api/projects/:projectId/origin` 는 인증 및 요금제 검증 후 실행된다 (`server/index.ts:969-1196`).
- 요청 유형
  - **멀티파트 파일 업로드**: `req.isMultipart()` 분기에서 단일 파일을 받고 `jobId` 등 부필드를 읽어 들인다 (`server/index.ts:1072-1135`).
  - **직접 텍스트 업로드**: JSON 본문 `content`를 그대로 저장하는 경로를 제공한다 (`server/index.ts:1157-1235`).
- 요청 거부 조건: 파일 누락, 비문자열 `content`, 추출 실패, 미지원 확장자, 10MB 초과 등은 4xx/5xx 응답으로 처리한다 (`server/services/origin/extractor.ts:21-238`, `server/index.ts:1093-1145`).

## 2. 파일 추출 및 정규화
`extractOriginFromUpload`가 확장자에 따라 추출기를 선택한다 (`server/services/origin/extractor.ts:72-238`).

| 확장자 | 추출기 | 라이브러리/방식 | 비고 |
| --- | --- | --- | --- |
| `.txt` | plain | UTF-8 디코딩 | 기본 경로 (`server/services/origin/extractor.ts:95`) |
| `.docx` | mammoth | `mammoth.extractRawText` | 스타일 무시 (`server/services/origin/extractor.ts:100`) |
| `.doc` | word-extractor | 임시 파일 + WordExtractor | 완료 후 파일 제거 (`server/services/origin/extractor.ts:170`) |
| `.pdf` | pdf-parse | 텍스트 레이어 추출 | Lazy-load (`server/services/origin/extractor.ts:181`) |
| `.epub` | epub2 | HTML 파싱→태그 제거 | 엔티티 디코드 포함 (`server/services/origin/extractor.ts:195`) |
| `.hwp/.hwpx` | hwp-extractor.py | PyHWP / hwp-extract | 타임아웃 30초, 친절한 오류 메시지 (`server/services/origin/extractor.ts:214`, `server/services/origin/hwp_extractor.py:41-78`) |

추출 결과는 `cleanText`와 `normalizeExtractedText`를 거쳐 공백/특수문자/개행을 정리하고, 문자·단어 수를 계산해 메타데이터에 포함한다 (`server/services/origin/extractor.ts:127-168`).

## 3. 저장 경로 및 메타데이터
- **Mongo `origin_files`**: 파일 메타데이터와 텍스트/바이너리를 upsert 한다 (`server/index.ts:998-1019`).
- **Mongo `translation_files`**: 최신 번역 파일 문서에 원문 텍스트와 메타데이터를 동기화한다 (`server/index.ts:1021-1031`).
- **Postgres `translationprojects.origin_file`**: 원문 ObjectId를 저장해 프로젝트 차원 스냅샷에서 참조한다 (`server/index.ts:1037-1043`).
- 저장 시점의 `updated_at`이 항상 갱신되며, 응답에는 정규화된 텍스트/통계가 포함된다 (`server/index.ts:1118-1130`).

## 4. TDM 역할 – Profiling 잡 처리
- 업로드 성공 후 인증된 사용자라면 `enqueueProfileAnalysisJob`이 Postgres 기반 `jobs` 테이블에 `type='profile'` 작업을 넣는다 (`server/index.ts:1052-1059`, `server/index.ts:2056-2105`).
- `startProfileWorker`는 2.5초 간격으로 큐를 폴링하여 `queued` → `running` 으로 상태를 전환하고 프로파일 분석을 수행한다 (`server/index.ts:3198-3368`). 이 루프가 원문 처리용 TDM(작업 매니저)의 역할을 담당한다.
- 작업 수행 단계
  1. 프로젝트/원문/번역 문서 로딩 후 분석 대상 텍스트 결정 (`server/index.ts:3226-3294`).
  2. GPT-5 기반 `analyzeDocumentProfile` 호출로 요약·의도·독자 포인트·번역 노트 JSON 생성 (`server/agents/profile/profileAgent.ts:420-703`).
  3. 결과를 `DocumentProfile`에 버전업 저장하고 품질/교정 참조 자료로 활용 (`server/index.ts:3306-3368`).
  4. 토큰 사용량을 `recordTokenUsage`에 기록하고 작업 상태를 `done`으로 마감 (`server/index.ts:3337-3357`).

### LLM 호출 구조 (GPT-5 Responses API)
- **모델**: 기본 `gpt-5`, fallback `gpt-5-mini`. ENV(`PROFILE_AGENT_MODEL`, `PROFILE_AGENT_VALIDATION_MODEL`)로 제어 (`server/agents/profile/profileAgent.ts:18-36`).
- **토큰 제어**: `max_output_tokens` 기본 1,600, 누락/파싱 오류 시 1.5~2배씩 증가하며 상한 4,800 (`server/agents/profile/profileAgent.ts:525-703`).
- **프롬프트 구성**
  - 시스템 프롬프트: 문학 비평가/번역 분석가 역할 정의, JSON Schema 구조 강제 (`server/agents/profile/profileAgent.ts:447-476`).
  - 사용자 프롬프트: 프로젝트/세그먼트 데이터와 작성 언어 지침, 번역 노트 스키마 등을 포함 (`server/agents/profile/profileAgent.ts:480-523`).
- **응답 파싱**: `safeExtractOpenAIResponse`가 Responses API 응답에서 JSON/텍스트를 추출, 파싱 실패 시 재시도 (`server/services/llm.ts:37-95`).
- **재시도**: `openai_response_incomplete`(토큰 부족) → 토큰 2배, `profile_invalid_json` → 토큰 1.5배로 늘려 재시도. 최대 시도 후 실패 시 에러 유지 (`server/agents/profile/profileAgent.ts:620-695`).
- **출력 정규화**: 요약은 단어 수 제한, 번역 노트는 캐릭터/지명/측정 단위 등 구조화 배열로 변환한다 (`server/agents/profile/profileAgent.ts:707-774`).

## 5. BDM 역할 – DocumentProfile 보존 및 메모리 갱신
- 생성된 프로파일은 `DocumentProfile` 컬렉션에 저장되며, 요약·메트릭·번역 노트·분석 메타(모델/토큰/재시도/트렁케이션 여부)가 함께 등록된다 (`server/index.ts:3306-3348`, `server/models/DocumentProfile.ts`).
- `buildMemorySeed`는 분석 결과와 원문 세그먼트를 기반으로 Project Memory에 캐릭터/용어/측정 단위/언어 특징 등의 초기 값을 병합하여 이후 번역 파이프라인이 같은 기준을 참조하도록 한다 (`server/index.ts:4268-4352`).
- 저장된 프로파일 정보와 메모리는 Proofread·Quality 프로세스에서 그대로 활용되며, `translation_notes`는 세그먼트별 번역 TDM에 직접 주입된다 (`server/index.ts:2440-2471`, `server/services/translation/memory.ts:52-101`).

## 6. 분석 로깅 및 복구
- 프로파일 워커는 각 작업마다 모델/verbosity/effort/max tokens/재시도 횟수/트렁케이션 여부를 포함한 로그를 남긴다 (`server/agents/profile/profileAgent.ts:605-760`).
- 실패 시 `jobs` 테이블 상태를 `failed`로 업데이트하고 오류 메시지를 저장함으로써 재처리/모니터링이 용이하다 (`server/index.ts:3349-3367`).
- `recordTokenUsage`는 `event_type="profile"` 로 토큰 사용량을 Project 단위에 누적한다 (`server/index.ts:3337-3357`).

## 7. 번역 준비용 세그멘테이션 (TDM 연계)
- 번역 파이프라인은 `segmentOriginText` 로 문단/문장 기반 세그먼트를 미리 생성한다 (`server/agents/translation/segmentationAgent.ts:106-183`).
- TDM은 이 세그먼트 정보와 프로파일 메모리(`translationNotes`)를 번역 큐 잡에 포함시켜 일관된 컨텍스트를 보장한다 (`server/index.ts:2406-2471`).
- 세그먼트는 paragraph/sentence 인덱스와 해시를 포함해 BDM이 최종 번역 세그먼트와 연결할 수 있도록 한다 (`server/agents/translation/segmentationAgent.ts:171-216`).

## 8. 오류 및 예외 처리
- 추출 실패: 사용자에게 “No textual content…” 오류 전달 (`server/index.ts:1136-1144`).
- 미지원 확장자: 400 오류 (`server/services/origin/extractor.ts:86`).
- Python 추출기 실패: 사용자 친화적 메시지를 포함한 500 오류 (`server/services/origin/extractor.ts:234-238`).
- 프로파일 LLM 실패: `jobs` 상태를 `failed`로 업데이트하고 로그에 원인 기록, 워커는 다음 작업으로 넘어간다 (`server/index.ts:3349-3367`).

---
**담당**
- Owner: Content Analysis Team (TDM: 프로파일 워커 운영, BDM: DocumentProfile 및 Project Memory 유지)
- 최신 업데이트: 2025-??-?? (GPT-5 Responses API 기반 원문 분석 반영)
